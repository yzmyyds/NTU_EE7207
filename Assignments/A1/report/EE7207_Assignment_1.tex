\documentclass[a4paper, 12pt, twoside]{article}
\usepackage{mathptmx}
\usepackage{enumitem} 
\usepackage[a4paper, hmarginratio=1:1, headheight=15pt, bottom=3.5cm, footskip=2cm]{geometry}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}  
\usepackage{cite}  
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}

\pagestyle{fancy}
\fancyhf{}
\newcommand{\headerstyle}[1]{{\footnotesize\itshape\color{gray} #1}}

\renewcommand{\sectionmark}[1]{\markboth{\thesection.\ #1}{}} 

\fancyfoot[C]{\headerstyle{\thepage}}
\fancyhead[L]{\headerstyle{EE7207 A1}} 
\fancyhead[RO]{\headerstyle{Yan Ziming}}
\fancyhead[RE]{\headerstyle{G2507084J}}

\renewcommand{\headrulewidth}{0.2pt} 
\renewcommand{\headrule}{\hbox to\headwidth{%
  \color{gray}\leaders\hrule height \headrulewidth\hfill}}

\renewcommand{\headrulewidth}{0.4pt}
\fancypagestyle{plain}{
    \fancyhf{}
    \fancyfoot[C]{\thepage}
    \renewcommand{\headrulewidth}{0pt}
}
\begin{document}

\begin{titlepage}
    \centering
    \includegraphics[width=10cm]{../../logo/Nanyang_Technological_University-Logo.wine.png}\par\vspace{1cm}
    {\scshape\LARGE Nanyang Technological University \par}
    \vspace{1.5cm}
    {\huge\bfseries EE7207 Neural Networks \& Deep Learning\par}
    \vspace{0.5cm}
    {\Large Assignment 1 Report\par}
    \vspace{1.5cm}
    {\large MSc in Electrical and Electronic Engineering\par}
    {\large Yan Ziming\par}
    {\large G2507084J\par}
    \vfill
    {\large \today \par}
    \thispagestyle{empty}
\end{titlepage}

\tableofcontents
\thispagestyle{empty} 

\cleardoublepage
\pagenumbering{arabic}

\section{Introduction}
The report introduces the process of classification with RBF 
neural network. The RBF contains three layers: input layer, 
hidden layer, and output layer. 

The key obstacle during the development is 
the selection of parameters, like Gaussian width ($\sigma$) 
and hidden layer neuron number. I used different methods to deal 
with these difficulties, such as SOM, Kmeans, and nonlinear 
optimization apart from basic process.

Briefly, I divided my design into two parts: (1) basic method to
develop basic RBF (2) improve parameters with backpropogation.
The final result reflect positive improvement.

\section{RBF Neural Network Frame}

\subsection{Input Layer}
The input layer of RBF is different from normal neural network, 
it just transmit the training data vector $\mathbf{x}=[x_1, x_2, 
..., x_n]^{T}$ to the hidden layer. In this stage, no mathematical 
transformations or learning processes occur.
\subsubsection{Data Analysis}
The datasets are provided as static files, namely \textit{data\_train.mat}, 
\textit{data\_test.mat} and \textit{label\_train.mat}; therefore, 
manual partitioning of training and testing sets is not required.

\textbf{Data Information:} 
\begin{table}[H] 
    \centering
    \caption{Sample Sets} 
    \begin{tabular}{lcc}
        \toprule
        \textit{Parameter} & \textit{Training Data}& \textit{Testing Data}  \\
        \midrule
        Number of Samples ($n$) & 301 & 50 \\
        Number of Features ($m$) & 33 & 33 \\
        Categories & 2 & 2 \\
        Missing Value & 0 & 0 \\ 
        Outlier Ratio (IQR)& 49.83\% & 54.00\%\\
        \bottomrule
    \end{tabular}
\end{table}
    
\begin{table}[H]
    \centering
    \caption{Label Set}
    \begin{tabular}{lc}
        \toprule
        \textit{Parameter} & \textit{Training Label} \\
        \midrule
        Number of Samples ($n$) & 301 \\
        Categories & \{-1, 1\}\\
        Distribution & [106, 195]\\
        Missing Value & 0\\
        \bottomrule
    \end{tabular}
\end{table}

I tried to seek for outliers with IQR metric and considered 
the sample which has outlier feature element as outlier sample to 
compute ratio. According to the data analysis, the outlier of sample set 
too large to be normal. Therefore, I plot boxplot to visualize 
the real distributions of training data in each feature.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/boxplot_features.png}
    \caption{Boxplot of Data\_train}
\end{figure}

According to figure, the main outliers come from several features. The 
distribution of data, like mean value and variation standardized and the 
range of value have normalized as well. There is no need to normalizztion 
and standarlization. 

Generally, the high dimension leads to extreme complexity in feature space.
It is the \textbf{Curse of Dimensionality}. Since the input layer need to 
transmit sample to hidden layer, the input layer should have 33 neurons.

\subsection{Hidden Layer}
The hidden layer is the computational core of the RBF network, responsible 
for mapping the input data into a new feature space based on the neurons in 
hidden layer. The RBF hidden layer utilizes radial basis functions to 
capture local responses. The output of the j-th hidden neuron, $\phi_j(\mathbf{x})$
is determined by the distance between the input vector $\mathbf{x}$ and a 
predefined center $\mathbf{c}_j$:
\begin{equation} 
    \phi_j(\mathbf{x}) = \exp \left( -\frac{|\mathbf{x} - \mathbf{c}_j|^2}{2\sigma_j^2} \right) 
\end{equation}

where $|\cdot|$ denotes the Euclidean norm, and $\sigma_j$ represents the Gaussian 
width (spread) of the j-th neuron. This local mapping ensures that only inputs near 
the center $\mathbf{c}_j$ produce a significant activation.

\subsubsection{Centers Selection}
The selection of centers $\mathbf{c}_j$ is the most critical step in designing the 
hidden layer, as they serve as the "prototypes" that define the localized receptive 
fields within the input space. Given the 33-dimensional complexity, a robust and 
systematic selection strategy is employed.

In this implementation, the K-means clustering algorithm is utilized as the primary 
method to partition the feature space. To determine the optimal number of hidden 
neurons K, the Elbow Method is performed by plotting the Within-Cluster Sum of Squares 
(WCSS) against a range of cluster counts. The "elbow" point of the curve is identified 
as the optimal balance between model complexity and error reduction, providing a 
quantitative basis for the hidden layer's scale.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/elbow_k_select.png}
    \caption{Cluster Number \& WCSS}
\end{figure}

Since the elbow occours at $k=6$ in the figure, I decide to use 6 hidden neuron to 
develop hidden layer.

To further validate the reliability of these centers in the presence of high-dimensional 
noise, a Self-Organizing Map (SOM) is used for auxiliary verification. While K-means 
efficiently minimizes local variance, the SOM provides a topological mapping that 
ensures the centers are not merely biased by dense clusters but also adequately cover t
he "outlier-heavy" boundaries identified in our data analysis. This dual-verification 
approach ensures that the prototype centers $\mathbf{c}_j$ maintain a representative 
coverage of the manifold.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/som_u_matrix_kmeans_centroids.png}
    \caption{Centroids on U\_matrix}
\end{figure}

The U\_matrix map reflected the $30 \times 30$ neurons' distances, the color darker, 
the distances of neuron to neighbors closer, meaning more possible to belongs to 
same cluster. Since the centroids computed by Kmeans clustering fall into the blue "valley",
the selection of 6 neurons in hidden layer roughly feasible.
\subsubsection{Gaussian Width}
The Gaussian width $\sigma$ determines the receptive field of each hidden neuron. 
Setting an appropriate $\sigma$ is a balancing act between sensitivity and generalization:
\begin{itemize} 
    \item \textbf{Addressing Dimensionality:} In a 33-dimensional space, the Euclidean 
    distance between points tends to be large and sparse. If $\sigma$ is too small, the 
    receptive fields will not overlap, causing the network to lose its interpolation 
    capability (the "dead neuron" problem). 
    \item \textbf{Robustness to Outliers:} Conversely, an excessively large $\sigma$ would 
    over-smooth the decision boundary, potentially blurring the distinction between the categories, 
    especially given the imbalanced distribution ([106, 195]). 
\end{itemize}

A common heuristic is used to set a global width: 
\begin{equation}
    \sigma=\frac{d_{max}}{\sqrt{2K}}
\end{equation}

where $d_{max}$is the maximum distance between chosen centers and K is the number of hidden 
units. This ensures that the hidden layer provides a continuous and smooth mapping 
across the entire input manifold.

In this experiment, the $\sigma$ I computed is 2.4604.

\subsection{Output Layer}
The output layer serves as the final stage of the RBF network, responsible for integrating the local features processed by the hidden layer into a coherent global decision. Structurally, it is a single-layer feed-forward network with a linear activation function.
\subsubsection{Structural Composition}

For this binary classification task, the output layer consists of a single neuron that performs a weighted summation. The inputs to this layer are the K activation values $[\phi_1(x),\phi_2(x),â€¦,\phi_K(x)]^T$
generated by the hidden neurons. The connection between the j-th hidden neuron and the output neuron is defined by a scalar weight $w_j$.

The total input to the output neuron, denoted as z, is calculated as: 
\begin{equation} 
    z = \sum_{j=1}^{K} w_j \phi_j(\mathbf{x}) + b 
\end{equation} 
\begin{equation}
    \mathbf{z}=\Phi_{bias}W
\end{equation}

In order to get appropriate prediction, the error should be minimized. With the Ordinary Least Square method, the weight matrix $W$ could be found. Subsequently, the label $\mathbf{z}$ could be predicted as well.

\subsubsection{Linear Summation and Decision Rule}
According to label data, the data belongs to two categories: $y\in\{-1,1\}$. Since the weight matrix $W$ and hidden layer output $\Phi$ are in float, the labels predicted will not follow these binary classes. Therefore, a signum function (or a threshold) should be applied to the raw result:
\begin{equation}
    y = \text{sgn}(z) = 
    \begin{cases}
        1 & \text{if } z \ge 0 \\
        -1 & \text{if } z < 0
    \end{cases}
\end{equation}

This simple decision rule converts the continuous output of the network into the discrete labels. The output $\mathbf{z} \in R^{n}$, n is the number of training sample, which means each input only output one number to represent their belongings. Therefore, the neuron number in output later should be 1.

\subsection{Nonlinear Optimization}
While the integration of K-means, SOM, and the heuristic formula $\sigma=\frac{d_{max}}{\sqrt{2K}}$
provides a statistically sound initialization for the hidden layer, these traditional methods often struggle with the high dimension data. In a 33-dimensional space, the predefined centers and the static Gaussian width may not capture the complex, non-linear decision boundaries with sufficient precision, potentially leading to sub-optimal mapping.

To address this, I leverage Non-linear Optimization via the PyTorch framework. Instead of treating the hidden layer parameters as fixed constants, I redefine the centers $\mathbf{c}_j$, widths $\sigma$ and weight matrix $W$
as trainable parameters. By implementing the Gradient Descent-based Backpropagation (BP) algorithm, the network can iteratively refine these coefficients to minimize the global loss function. This transition from heuristic initialization to gradient-based fine-tuning allows the RBF neurons to adaptively "shift" and "resize" their receptive fields, significantly improving the model's accuracy and robustness against the high outlier ratio observed in our data.
\section{Results Analysis}
\subsection{Baseline}
The baseline reflects the result before the nonlinear optimization.
\subsubsection{Prediction Result}
\begin{table}[H]
    \centering
    \caption{Prediction Class Label}
    \begin{tabular}{|l|c|}
        \hline
        \textit{Sample Index}&\textit{Class Label}\\
        \hline
        0-9   & -1  1 -1 -1  1  1 -1  1 -1 -1\\
        \hline
        10-19 & -1  1 -1 -1 -1 -1 -1  1 -1 -1\\
        \hline
        20-29 &  1  1  1 -1 -1  1 -1  1 -1  1\\
        \hline
        30-39 & -1  1 -1  1  1  1  1  1  1  1\\
        \hline
        40-49 &  1 -1  1  1 -1  1  1  1  1  1\\
        \hline
    \end{tabular}
\end{table}
\subsubsection{Neuron Coverage}
One important method to check whether the neuron and $\sigma$ selected are feasible is the coverage of neurons. Since the dimension of data is too high to plot, I decrease the dimension with PCA and plot the corresponding neuron coverage. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/coverage_check_pca.png}
    \caption{Coverage of Centroids}
\end{figure}

\subsection{After Backpropogation}
\subsubsection{Prediction Result}
\begin{table}[H]
    \centering
    \caption{Prediction with Optimization}
    \begin{tabular}{|l|c|}
        \hline
        \textit{Sample Index}&\textit{Class Label}\\
        \hline
        0-9   & -1  1  1 -1  1  1 -1  1  1 -1\\
        \hline
        10-19 & -1  1 -1 -1 -1 -1 -1  1  1 -1\\
        \hline
        20-29 &  1  1  1 -1 -1  1 -1  1 -1  1\\
        \hline
        30-39 & -1  1 -1  1  1  1  1  1  1  1\\
        \hline
        40-49 &  1  1  1  1  1  1  1  1  1  1\\
        \hline
    \end{tabular}
\end{table}
\subsubsection{Coverage \& Loss Curve}
For the backpropogation, I use \textit{torch.nn} and set learning rate = 0.001, epoch = 2000. The following figures shows the training loss and neuron coverage after optimization. From loss curve, the BP does reduces the loss function with epoch increasing.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/training_loss_curve.png}
    \caption{Training Loss}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../fig/final_centres_backprop.png}
    \caption{Center Coverage After Optimization}
\end{figure}

\subsection{Comparition}
To evaluate the RBF\_nn roughly and statistically, I predict the training label to get metrics, comparing with given label. The results of baseline and optimization are shown in table.
\begin{table}[H]
    \centering
    \caption{Metrics of Model}
    \begin{tabular}{lcc}
        \toprule
        \textit{Metric}&\textit{Base}&\textit{Optimized}\\
        \midrule
        Accuracy& 0.920&0.983\\
        Precision &0.907&0.985\\
        Recall &0.927&0.979\\
        F1 Score& 0.915&0.982\\
        Poor coverage number& 36/301&0/301\\
        \bottomrule
    \end{tabular}
\end{table}

By comparing the metrics and coverage visualizations, it is evident that non-linear optimization significantly enhances the model's fit to the training data and refines the spatial distribution of hidden neurons. However, in the absence of ground-truth testing labels, it remains inconclusive whether this performance gain represents a genuine improvement in predictive power or an artifact of over-fitting. The results suggest that while the network has successfully minimized the empirical risk on the training manifold, its generalization capability requires further cross-validation or regularization audits.
\newpage
\section{Appendix}
\subsection{Core Implementation}
\subsubsection{K-means Algorithm}
\begin{figure*}[H]
    \includegraphics[width=0.8\textwidth]{../fig/Kmeans.png}
\end{figure*}
\subsubsection{RBF Neuron Network Configuation}
\begin{figure*}[H]
    \includegraphics[width=0.8\textwidth]{../fig/RBFnn.png}
\end{figure*}
\subsubsection{SOM for U-matrix Map} 
\begin{figure*}[H]
    \includegraphics[width=0.8\textwidth]{../fig/SOM.png}
\end{figure*}
\subsubsection{Backpropagation Initialization \& Forward Training}
\begin{figure*}[H]
    \includegraphics[width=0.8\textwidth]{../fig/BP.png}
\end{figure*}

The full code for this assignment are stored in github repository. URL: \url{https://github.com/yzmyyds/NTU_EE7207}
\subsection{Dependency List}
\begin{itemize}
    \item Python: 3.11.13
    \item PyTorch: 2.10.0
    \item Scikit-learn: 1.7.2
    \item Matplotlib: 3.10.6
    \item SciPy: 1.16.1
    \item NumPy: 2.2.6
\end{itemize}
\end{document}