\documentclass[a4paper, 12pt, twoside]{article}
\usepackage{mathptmx}
\usepackage{enumitem} 
\usepackage[a4paper, hmarginratio=1:1, headheight=15pt, bottom=3.5cm, footskip=2cm]{geometry}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}  
\usepackage{cite}  
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}

\pagestyle{fancy}
\fancyhf{}
\newcommand{\headerstyle}[1]{{\footnotesize\itshape\color{gray} #1}}

\renewcommand{\sectionmark}[1]{\markboth{\thesection.\ #1}{}} 

\fancyfoot[C]{\headerstyle{\thepage}}
\fancyhead[L]{\headerstyle{EE7207 A1}} 
\fancyhead[RO]{\headerstyle{Yan Ziming}}
\fancyhead[RE]{\headerstyle{G2507084J}}

\renewcommand{\headrulewidth}{0.2pt} 
\renewcommand{\headrule}{\hbox to\headwidth{%
  \color{gray}\leaders\hrule height \headrulewidth\hfill}}

\renewcommand{\headrulewidth}{0.4pt}
\fancypagestyle{plain}{
    \fancyhf{}
    \fancyfoot[C]{\thepage}
    \renewcommand{\headrulewidth}{0pt}
}
\begin{document}

\begin{titlepage}
    \centering
    \includegraphics[width=10cm]{../../logo/Nanyang_Technological_University-Logo.wine.png}\par\vspace{1cm}
    {\scshape\LARGE Nanyang Technological University \par}
    \vspace{1.5cm}
    {\huge\bfseries EE7207 Neural Networks \& Deep Learning\par}
    \vspace{0.5cm}
    {\Large Assignment 1 Report\par}
    \vspace{1.5cm}
    {\large MSc in Electrical and Electronic Engineering\par}
    {\large Yan Ziming\par}
    {\large G2507084J\par}
    \vfill
    {\large \today \par}
    \thispagestyle{empty}
\end{titlepage}

\tableofcontents
\thispagestyle{empty} 

\cleardoublepage
\pagenumbering{arabic}

\section{Introduction}
The report introduces the process of classification with RBF 
neural network. The RBF contains three layers: input layer, 
hidden layer, and output layer. 

The key obstacle during the development is 
the selection of parameters, like Gaussian width ($\sigma$) 
and hidden layer neuron number. I used different methods to deal 
with these difficulties, such as SOM, Kmeans, and nonlinear 
optimization apart from basic process.

Briefly, I divided my design into two parts: (1) basic method to
develop basic RBF (2) improve parameters with backpropogation.
The final result reflect positive improvement.

\section{RBF Neural Network Frame}

\subsection{Input Layer}
The input layer of RBF is different from normal neural network, 
it just transmit the training data vector $\mathbf{x}=[x_1, x_2, 
..., x_n]^{T}$ to the hidden layer. In this stage, no mathematical 
transformations or learning processes occur.
\subsubsection{Data Analysis}
The datasets are provided as static files, namely \textit{data\_train.mat}, 
\textit{data\_test.mat} and \textit{label\_train.mat}; therefore, 
manual partitioning of training and testing sets is not required.

\textbf{Data Information:} 

{\centering
    \begin{tabular}{lcc}
            \toprule
            \textit{Parameter} & \textit{Training Data}& \textit{Testing Data}  \\
            \midrule
            Number of Samples ($n$) & 301 & 50 \\
            Number of Features ($m$) & 33 & 33 \\
            Distribution (IQR) & &\\
            Categories & 2 & 2 \\
            \bottomrule
    \end{tabular} \par}
\subsubsection{Preprocessing}

\subsection{Hidden Layer}

\subsubsection{Kmeans}

\subsubsection{SOM}

\subsection{Output Layer}

\section{Results Analysis}

\section{Check \& Improvement}

\section{Conclusion}

\end{document}